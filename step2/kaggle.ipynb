{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6b195e",
   "metadata": {},
   "source": [
    "# DeBERTa-v3 Inference Pipeline\n",
    "RAM-optimized inference for Kaggle CPU environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b3630",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/kaggle/input/deberta-v3-response-classifier/transformers/default/2\"\n",
    "TEST_CSV = \"/kaggle/input/llm-classification-finetuning/test.csv\"\n",
    "OUT_SUBMISSION = \"submission.csv\"\n",
    "\n",
    "NUM_THREADS = 8\n",
    "BATCH_SIZE = 1\n",
    "MAX_LENGTH = 256\n",
    "USE_FLOAT16 = True\n",
    "FLUSH_EVERY_N_BATCHES = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad569aad",
   "metadata": {},
   "source": [
    "## Setup Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f020268",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(NUM_THREADS)\n",
    "print(f\"[INFO] device={device}, threads={torch.get_num_threads()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace9bdf",
   "metadata": {},
   "source": [
    "## Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Loading tokenizer and model (low_cpu_mem_usage=True)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True, low_cpu_mem_usage=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"[INFO] Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda43009",
   "metadata": {},
   "source": [
    "## Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192302be",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = None\n",
    "try:\n",
    "    clf = joblib.load(os.path.join(MODEL_PATH, \"deberta_classifier.pkl\"))\n",
    "    has_clf = True\n",
    "    print(\"[INFO] Classifier loaded.\")\n",
    "except Exception as e:\n",
    "    has_clf = False\n",
    "    print(\"[WARN] Classifier not found or failed to load. Will save embeddings per-batch instead.\")\n",
    "    print(\"       Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f550584",
   "metadata": {},
   "source": [
    "## Define Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts_get_cls_embeddings(texts):\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    for k, v in encoded.items():\n",
    "        encoded[k] = v.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "        cls = outputs.last_hidden_state[:, 0, :].cpu()\n",
    "        if USE_FLOAT16:\n",
    "            cls = cls.to(torch.float16)\n",
    "        else:\n",
    "            cls = cls.to(torch.float32)\n",
    "        arr = cls.numpy()\n",
    "    \n",
    "    del encoded, outputs, cls\n",
    "    gc.collect()\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23750af",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58991cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Loading test CSV...\")\n",
    "test = pd.read_csv(TEST_CSV)\n",
    "n = len(test)\n",
    "print(f\"[INFO] Test rows: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aae545",
   "metadata": {},
   "source": [
    "## Process Batches & Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rows = []\n",
    "flush_count = 0\n",
    "\n",
    "for start in tqdm(range(0, n, BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    end = min(start + BATCH_SIZE, n)\n",
    "    batch_df = test.iloc[start:end]\n",
    "    \n",
    "    batch_texts_a = (batch_df[\"prompt\"].astype(str) + \" \" + batch_df[\"response_a\"].astype(str)).tolist()\n",
    "    batch_texts_b = (batch_df[\"prompt\"].astype(str) + \" \" + batch_df[\"response_b\"].astype(str)).tolist()\n",
    "\n",
    "    emb_a = encode_texts_get_cls_embeddings(batch_texts_a)\n",
    "    emb_b = encode_texts_get_cls_embeddings(batch_texts_b)\n",
    "\n",
    "    for i, idx in enumerate(batch_df.index):\n",
    "        combined = np.concatenate([emb_a[i], emb_b[i]], axis=0)\n",
    "        if has_clf:\n",
    "            proba = clf.predict_proba(combined.reshape(1, -1))[0]\n",
    "            out_rows.append({\n",
    "                \"id\": int(test.at[idx, \"id\"]),\n",
    "                \"winner_model_a\": float(proba[0]),\n",
    "                \"winner_model_b\": float(proba[1]),\n",
    "                \"winner_tie\": float(proba[2]),\n",
    "            })\n",
    "        else:\n",
    "            emb_path = f\"emb_sample_{idx}.npy\"\n",
    "            np.save(emb_path, combined)\n",
    "    \n",
    "    if len(out_rows) >= FLUSH_EVERY_N_BATCHES:\n",
    "        if not os.path.exists(OUT_SUBMISSION):\n",
    "            pd.DataFrame(out_rows).to_csv(OUT_SUBMISSION, index=False, mode=\"w\")\n",
    "        else:\n",
    "            pd.DataFrame(out_rows).to_csv(OUT_SUBMISSION, index=False, mode=\"a\", header=False)\n",
    "        out_rows = []\n",
    "        flush_count += 1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af15d63",
   "metadata": {},
   "source": [
    "## Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2802c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_clf:\n",
    "    if len(out_rows) > 0:\n",
    "        if not os.path.exists(OUT_SUBMISSION):\n",
    "            pd.DataFrame(out_rows).to_csv(OUT_SUBMISSION, index=False, mode=\"w\")\n",
    "        else:\n",
    "            pd.DataFrame(out_rows).to_csv(OUT_SUBMISSION, index=False, mode=\"a\", header=False)\n",
    "    print(f\"[INFO] Predictions saved to {OUT_SUBMISSION}\")\n",
    "else:\n",
    "    print(\"[INFO] Embeddings saved per-sample as 'emb_sample_<index>.npy' in the working directory.\")\n",
    "    print(\"       If you want a single combined embeddings file, you can concat them later by loading those .npy files sequentially.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
