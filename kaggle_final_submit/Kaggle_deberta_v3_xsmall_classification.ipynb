{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":631413,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":475870,"modelId":491776}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:07:45.040691Z","iopub.execute_input":"2025-11-06T05:07:45.041040Z","iopub.status.idle":"2025-11-06T05:07:46.743415Z","shell.execute_reply.started":"2025-11-06T05:07:45.040989Z","shell.execute_reply":"2025-11-06T05:07:46.742538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\nclass TIDAutoBertClassification(nn.Module):\n    def __init__(\n        self,\n        model_name = \"google/mobilebert-uncased\",\n        pooling: str = \"cls\",  # one of {\"cls\", \"mean\", \"max\"}\n        dropout: float = 0.1,\n        num_labels: int = 3,\n    ):\n        super().__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n\n        hidden = self.model.config.hidden_size\n        self.init_param = {\"pooling\":pooling, \"dropout\":dropout, \"num_labels\":num_labels}\n        self.pooling = pooling.lower()\n        self.dropout = nn.Dropout(dropout)\n\n        # Concatenate pooled A and B: 2 * hidden\n        self.classifier = nn.Sequential(\n            nn.Linear(2 * hidden, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels),\n        )\n\n    def _pool(self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n        if self.pooling == \"cls\":\n            return last_hidden_state[:, 0]  # [CLS]-like (actually first token)\n        elif self.pooling == \"max\":\n            # Mask padded positions to very negative before max\n            mask = attention_mask.unsqueeze(-1).bool()\n            masked = last_hidden_state.masked_fill(~mask, -1e9)\n            return masked.max(dim=1).values\n        else:  # mean (mask-aware)\n            mask = attention_mask.unsqueeze(-1)  # (B, L, 1)\n            summed = (last_hidden_state * mask).sum(dim=1)\n            denom = mask.sum(dim=1).clamp(min=1)\n            return summed / denom\n\n    def encode(self, **inputs) -> torch.Tensor:\n        outputs = self.model(**inputs)\n        return outputs.last_hidden_state\n\n    def forward(\n        self,\n        input_ids_a: torch.Tensor,\n        attention_mask_a: torch.Tensor,\n        input_ids_b: torch.Tensor,\n        attention_mask_b: torch.Tensor,\n    ):\n        # Run shared backbone on A\n        h_a = self.encode(\n            input_ids=input_ids_a,\n            attention_mask=attention_mask_a,\n        )\n        # Run shared backbone on B\n        h_b = self.encode(\n            input_ids=input_ids_b,\n            attention_mask=attention_mask_b,\n        )\n\n        # Token-level -> sequence-level pooling\n        z_a = self._pool(h_a, attention_mask_a)\n        z_b = self._pool(h_b, attention_mask_b)\n\n        # Concatenate and classify\n        z = torch.cat([z_a, z_b], dim=-1)\n        z = self.dropout(z)\n        logits = self.classifier(z)\n\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:11:30.280407Z","iopub.execute_input":"2025-11-06T05:11:30.280952Z","iopub.status.idle":"2025-11-06T05:11:30.292745Z","shell.execute_reply.started":"2025-11-06T05:11:30.280925Z","shell.execute_reply":"2025-11-06T05:11:30.291559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load(path, device, model_class, tokenizer_class):\n    state = torch.load(os.path.join(path, \"checkpoint.ckpt\"), map_location=device)\n    pooling = state[\"init_param\"][\"pooling\"]\n    dropout = state[\"init_param\"][\"dropout\"]\n    num_labels = state[\"init_param\"][\"num_labels\"]\n\n    model = model_class(\n        model_name = path,\n        pooling=pooling,\n        dropout=dropout,\n        num_labels=num_labels\n    )\n\n    model.init_param = state[\"init_param\"]\n    model.pooling = state[\"pooling\"]\n    model.dropout.load_state_dict(state[\"dropout_state_dict\"])\n    model.classifier.load_state_dict(state[\"classifier_state_dict\"])\n\n    # 4. tokenizer 로드\n    tokenizer = tokenizer_class.from_pretrained(path)\n\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:11:35.487926Z","iopub.execute_input":"2025-11-06T05:11:35.488522Z","iopub.status.idle":"2025-11-06T05:11:35.496982Z","shell.execute_reply.started":"2025-11-06T05:11:35.488486Z","shell.execute_reply":"2025-11-06T05:11:35.495816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nckpt_path = \"/kaggle/input/deberta-v3-xsmall-classification/pytorch/default/1/\"\nmodel, tokenizer = load(ckpt_path, device=device,\n                        model_class=TIDAutoBertClassification,\n                        tokenizer_class=AutoTokenizer)\nmodel.to(device)\nprint(\"model load!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:11:46.411106Z","iopub.execute_input":"2025-11-06T05:11:46.411457Z","iopub.status.idle":"2025-11-06T05:12:17.676595Z","shell.execute_reply.started":"2025-11-06T05:11:46.411431Z","shell.execute_reply":"2025-11-06T05:12:17.675331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\nTEST_PATH=\"/kaggle/input/llm-classification-finetuning/test.csv\"\nOUTPUT_PATH=\"/kaggle/working/submission.csv\"\ndf = pd.read_csv(TEST_PATH)\nids = df[\"id\"]\nprompt = df[\"prompt\"]\nresponse_a = df[\"response_a\"]\nresponse_b = df[\"response_b\"]\nprint(df.head())\n\nresults = []\nfor i in range(0, len(prompt)):\n  k = ids[i]\n  p = prompt[i]\n  a = response_a[i]\n  b = response_b[i]\n\n  input1 = p + \"[SEP]\" + a\n  input2 = p + \"[SEP]\" + b\n  t1 = tokenizer(input1,padding=True,truncation=True,max_length=512,return_tensors=\"pt\")\n  t2 = tokenizer(input2,padding=True,truncation=True,max_length=512,return_tensors=\"pt\")\n  input_ids1 = t1.input_ids\n  input_ids2 = t2.input_ids\n  attention_mask1 = t1.attention_mask\n  attention_mask2 = t2.attention_mask\n\n  input_ids1 = input_ids1.to(device)\n  attention_mask1 = attention_mask1.to(device)\n  input_ids2 = input_ids2.to(device)\n  attention_mask2 = attention_mask2.to(device)\n\n  with torch.no_grad():\n    logits = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n    probs = F.softmax(logits, dim=-1).cpu().numpy().flatten()\n  results.append({\n    \"id\": k,\n    \"winner_model_a\": probs[0],\n    \"winner_model_b\": probs[1],\n    \"winner_tie\": probs[2]\n  })\n\n# DataFrame 변환 및 저장\nsubmission_df = pd.DataFrame(results)\nsubmission_df.to_csv(OUTPUT_PATH, index=False)\n\nprint(f\"✅ '{OUTPUT_PATH}' 파일이 생성되었습니다.\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:22:49.444534Z","iopub.execute_input":"2025-11-06T05:22:49.444936Z","iopub.status.idle":"2025-11-06T05:22:52.264727Z","shell.execute_reply.started":"2025-11-06T05:22:49.444910Z","shell.execute_reply":"2025-11-06T05:22:52.263219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}